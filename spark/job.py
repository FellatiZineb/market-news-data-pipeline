#!/usr/bin/env python3
"""
===========================================
SPARK JOB - Analyse de Sentiments Market News
===========================================
Objectif : Charger les donnÃ©es depuis Elasticsearch et effectuer
des transformations distribuÃ©es avec Spark.

BarÃ¨me Partie 5 (20 pts) :
- Charger les donnÃ©es depuis Elasticsearch
- Appliquer des transformations (calculs, agrÃ©gations)
- Exporter les rÃ©sultats en JSON et CSV

Auteur : M2 DataScale - UniversitÃ© Paris-Saclay
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, avg, count, sum as spark_sum, max as spark_max, 
    min as spark_min, round as spark_round, explode, 
    from_json, to_date, year, month, dayofmonth
)
from pyspark.sql.types import StructType, StructField, StringType, FloatType, ArrayType
import os

# ========================================
# CONFIGURATION
# ========================================

# Elasticsearch (avec fallback pour portabilitÃ©)
ES_HOST = os.getenv("ES_HOST", "elasticsearch")  # Nom du conteneur / host (localhost pour Windows)
ES_PORT = os.getenv("ES_PORT", "9200")           # Port interne Docker (9200) ou 9201si sur host
ES_INDEX = "market-news-*"

# Spark Master (cluster Docker ou local[*] pour tests)
SPARK_MASTER = os.getenv("SPARK_MASTER", "local[*]")

# RÃ©pertoire de sortie (adaptÃ© Ã  ton env : /data ou ./spark-output)
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "spark-output")

# ========================================
# INITIALISATION SPARK
# ========================================

print("=" * 60)
print("ğŸ“Š SPARK JOB - Analyse de Sentiments Market News")
print("=" * 60)

spark = SparkSession.builder \
    .appName("MarketNewsSentimentAnalysis") \
    .master(SPARK_MASTER) \
    .config("spark.es.nodes", ES_HOST) \
    .config("spark.es.port", ES_PORT) \
    .config("spark.es.nodes.wan.only", "false") \
    .config("spark.es.index.read.missing.as.empty", "true") \
    .config("spark.jars.packages", "org.elasticsearch:elasticsearch-spark-30_2.12:8.12.0") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print(f"âœ… Spark Session crÃ©Ã©e : {spark.version}")
print(f"ğŸ”— Elasticsearch : {ES_HOST}:{ES_PORT}")
print(f"ğŸ“‚ Index : {ES_INDEX}\n")

# ========================================
# 1. CHARGEMENT DES DONNÃ‰ES
# ========================================

print("ğŸ“¥ Chargement des donnÃ©es depuis Elasticsearch...")

df = spark.read \
    .format("org.elasticsearch.spark.sql") \
    .option("es.nodes", ES_HOST) \
    .option("es.port", ES_PORT) \
    .option("es.read.field.as.array.include", "ticker_sentiments") \
    .load(ES_INDEX)

total_count = df.count()
print(f"âœ… {total_count} documents chargÃ©s\n")

if total_count == 0:
    print("âš ï¸  AUCUNE DONNÃ‰E TROUVÃ‰E - VÃ©rifier que le producer a envoyÃ© des donnÃ©es")
    spark.stop()
    exit(1)

# Affichage du schÃ©ma
print("ğŸ“‹ SchÃ©ma des donnÃ©es :")
df.printSchema()

# AperÃ§u des donnÃ©es
print("\nğŸ“„ AperÃ§u des 3 premiers documents :")
df.select("title", "sentiment_label", "sentiment_score", "source", "published_at").show(3, truncate=False)

# ========================================
# 2. TRANSFORMATIONS ET ANALYSES
# ========================================

print("\n" + "=" * 60)
print("ğŸ”¬ ANALYSE 1 : Statistiques Globales du Sentiment")
print("=" * 60)

global_stats = df.select(
    count("*").alias("total_articles"),
    spark_round(avg("sentiment_score"), 4).alias("avg_sentiment_score"),
    spark_round(spark_min("sentiment_score"), 4).alias("min_sentiment"),
    spark_round(spark_max("sentiment_score"), 4).alias("max_sentiment")
).collect()[0]

print(f"""
ğŸ“Š Statistiques Globales :
   - Nombre total d'articles : {global_stats['total_articles']}
   - Score moyen de sentiment : {global_stats['avg_sentiment_score']}
   - Score minimum : {global_stats['min_sentiment']}
   - Score maximum : {global_stats['max_sentiment']}
""")

# Sauvegarde JSON
global_stats_df = df.select(
    count("*").alias("total_articles"),
    spark_round(avg("sentiment_score"), 4).alias("avg_sentiment_score"),
    spark_round(spark_min("sentiment_score"), 4).alias("min_sentiment"),
    spark_round(spark_max("sentiment_score"), 4).alias("max_sentiment")
)

global_stats_df.coalesce(1).write.mode("overwrite").json(f"{OUTPUT_DIR}/global_stats")
print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s : {OUTPUT_DIR}/global_stats/")

# ========================================
print("\n" + "=" * 60)
print("ğŸ”¬ ANALYSE 2 : Sentiment par Source de Presse")
print("=" * 60)

sentiment_by_source = df.groupBy("source") \
    .agg(
        count("*").alias("article_count"),
        spark_round(avg("sentiment_score"), 4).alias("avg_sentiment"),
        spark_round(spark_min("sentiment_score"), 4).alias("min_sentiment"),
        spark_round(spark_max("sentiment_score"), 4).alias("max_sentiment")
    ) \
    .orderBy(col("article_count").desc())

print("ğŸ“° Top 10 Sources par nombre d'articles :")
sentiment_by_source.show(10, truncate=False)

# Sauvegarde CSV + JSON
sentiment_by_source.coalesce(1).write.mode("overwrite") \
    .option("header", "true") \
    .csv(f"{OUTPUT_DIR}/sentiment_by_source_csv")

sentiment_by_source.coalesce(1).write.mode("overwrite") \
    .json(f"{OUTPUT_DIR}/sentiment_by_source_json")

print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s :")
print(f"   - CSV : {OUTPUT_DIR}/sentiment_by_source_csv/")
print(f"   - JSON : {OUTPUT_DIR}/sentiment_by_source_json/")

# ========================================
print("\n" + "=" * 60)
print("ğŸ”¬ ANALYSE 3 : Distribution des Labels de Sentiment")
print("=" * 60)

sentiment_distribution = df.groupBy("sentiment_label") \
    .agg(
        count("*").alias("count"),
        spark_round(avg("sentiment_score"), 4).alias("avg_score")
    ) \
    .orderBy(col("count").desc())

print("ğŸ·ï¸  Distribution des sentiments :")
sentiment_distribution.show(truncate=False)

# Sauvegarde
sentiment_distribution.coalesce(1).write.mode("overwrite") \
    .option("header", "true") \
    .csv(f"{OUTPUT_DIR}/sentiment_distribution")

print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s : {OUTPUT_DIR}/sentiment_distribution/")

# ========================================
print("\n" + "=" * 60)
print("ğŸ”¬ ANALYSE 4 : Sentiments par Ticker (AAPL, MSFT, GOOGL)")
print("=" * 60)

# SchÃ©ma pour parser ticker_sentiments
ticker_schema = ArrayType(StructType([
    StructField("ticker", StringType(), True),
    StructField("relevance_score", StringType(), True),
    StructField("ticker_sentiment_score", StringType(), True),
    StructField("ticker_sentiment_label", StringType(), True)
]))

# Exploser le tableau ticker_sentiments
df_exploded = df.select(
    "title",
    explode("ticker_sentiments").alias("ticker_data")
).select(
    "title",
    col("ticker_data.ticker").alias("ticker"),
    col("ticker_data.ticker_sentiment_score").cast("float").alias("ticker_sentiment_score"),
    col("ticker_data.ticker_sentiment_label").alias("ticker_sentiment_label"),
    col("ticker_data.relevance_score").cast("float").alias("relevance_score")
)

# AgrÃ©gation par ticker
ticker_analysis = df_exploded.groupBy("ticker") \
    .agg(
        count("*").alias("mention_count"),
        spark_round(avg("ticker_sentiment_score"), 4).alias("avg_sentiment"),
        spark_round(avg("relevance_score"), 4).alias("avg_relevance")
    ) \
    .orderBy(col("mention_count").desc())

print("ğŸ“ˆ Analyse par Ticker :")
ticker_analysis.show(10, truncate=False)

# Sauvegarde
ticker_analysis.coalesce(1).write.mode("overwrite") \
    .option("header", "true") \
    .csv(f"{OUTPUT_DIR}/ticker_analysis")

print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s : {OUTPUT_DIR}/ticker_analysis/")

# ========================================
print("\n" + "=" * 60)
print("ğŸ”¬ ANALYSE 5 : Articles Positifs vs NÃ©gatifs")
print("=" * 60)

positive_news = df.filter(col("sentiment_label") == "Bullish") \
    .select("title", "sentiment_score", "source", "published_at") \
    .orderBy(col("sentiment_score").desc())

negative_news = df.filter(col("sentiment_label") == "Bearish") \
    .select("title", "sentiment_score", "source", "published_at") \
    .orderBy(col("sentiment_score").asc())

print(f"\nğŸ“ˆ Top 5 Articles POSITIFS (Bullish) :")
positive_news.show(5, truncate=False)

print(f"\nğŸ“‰ Top 5 Articles NÃ‰GATIFS (Bearish) :")
negative_news.show(5, truncate=False)

# Sauvegarde
positive_news.coalesce(1).write.mode("overwrite") \
    .option("header", "true") \
    .csv(f"{OUTPUT_DIR}/positive_news")

negative_news.coalesce(1).write.mode("overwrite") \
    .option("header", "true") \
    .csv(f"{OUTPUT_DIR}/negative_news")

print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s :")
print(f"   - Positifs : {OUTPUT_DIR}/positive_news/")
print(f"   - NÃ©gatifs : {OUTPUT_DIR}/negative_news/")

# ========================================
# RÃ‰CAPITULATIF
# ========================================

print("\n" + "=" * 60)
print("âœ… TRAITEMENT SPARK TERMINÃ‰ AVEC SUCCÃˆS")
print("=" * 60)
print(f"""
ğŸ“Š RÃ©sumÃ© des analyses effectuÃ©es :
   1. Statistiques globales du sentiment
   2. Sentiment moyen par source de presse
   3. Distribution des labels de sentiment
   4. Analyse des sentiments par ticker (AAPL, MSFT, GOOGL)
   5. Extraction des articles positifs et nÃ©gatifs

ğŸ“ Tous les rÃ©sultats sont exportÃ©s dans : {OUTPUT_DIR}/

ğŸ“¦ Formats disponibles :
   - JSON (pour rÃ©ingestion)
   - CSV (pour Excel, Pandas)

ğŸ¯ Prochaines Ã©tapes :
   - Copier les rÃ©sultats depuis le conteneur Docker
   - Inclure dans le rapport final
   - CrÃ©er les visualisations Kibana (Partie 4)
""")

# ArrÃªt propre de Spark
spark.stop()
print("ğŸ›‘ Spark Session fermÃ©e\n")
