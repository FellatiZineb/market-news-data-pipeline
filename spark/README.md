# ğŸš€ Spark Job - Guide d'Utilisation

## ğŸ“‹ PrÃ©requis

1. âœ… Docker Desktop lancÃ©
2. âœ… Stack Docker Compose active (`docker compose up -d`)
3. âœ… DonnÃ©es indexÃ©es dans Elasticsearch (via producer â†’ Kafka â†’ Logstash)
4. âœ… Conteneurs Spark disponibles (selon ta fiche technique)

---

## ğŸ”§ Configuration

Le job Spark se connecte Ã  :
- **Elasticsearch** : `elasticsearch:9200` (conteneur Docker)
- **Spark Master** : `spark://spark-master:7077`
- **RÃ©pertoire de sortie** : `/data/spark-output` (montÃ© sur Windows : `C:\Users\benal\spark-tp`)

---

## ğŸ¯ Ce que fait le Job

Le script `job.py` effectue **5 analyses distribuÃ©es** :

### 1. **Statistiques Globales**
- Nombre total d'articles
- Score moyen de sentiment
- Min/Max des scores

### 2. **Sentiment par Source de Presse**
- AgrÃ©gation par source (Reuters, Bloomberg, etc.)
- Moyenne des scores par source
- Nombre d'articles par source

### 3. **Distribution des Labels**
- Compte des articles Bullish / Bearish / Neutral
- Score moyen par label

### 4. **Analyse par Ticker**
- Sentiments pour AAPL, MSFT, GOOGL
- Score moyen et relevance par ticker
- Nombre de mentions

### 5. **Top Articles Positifs/NÃ©gatifs**
- 5 articles les plus positifs
- 5 articles les plus nÃ©gatifs

---

## ğŸš€ ExÃ©cution du Job

### Option A : Depuis Windows (RecommandÃ©)

Si tu as Python + PySpark installÃ© localement :

```powershell
cd C:\Users\benal\OneDrive\Bureau\M2 DataScale\Partie 2\indexation\Projet\market-news-data-pipeline\spark

# ExÃ©cuter le job
python job.py
```

### Option B : Dans le Conteneur Spark

```powershell
# Copier le script dans le conteneur (si pas dÃ©jÃ  montÃ©)
docker cp job.py spark-master:/opt/spark/work-dir/

# ExÃ©cuter dans le conteneur
docker exec -it spark-master bash

# Puis dans le conteneur :
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.elasticsearch:elasticsearch-spark-30_2.12:8.12.0 \
  /opt/spark/work-dir/job.py
```

---

## ğŸ“‚ RÃ©cupÃ©ration des RÃ©sultats

Les rÃ©sultats sont exportÃ©s dans `/data/spark-output/` (conteneur) = `C:\Users\benal\spark-tp\spark-output\` (Windows).

### Structure des fichiers :

```
C:\Users\benal\spark-tp\spark-output\
â”œâ”€â”€ global_stats/
â”‚   â””â”€â”€ *.json
â”œâ”€â”€ sentiment_by_source_csv/
â”‚   â””â”€â”€ *.csv
â”œâ”€â”€ sentiment_by_source_json/
â”‚   â””â”€â”€ *.json
â”œâ”€â”€ sentiment_distribution/
â”‚   â””â”€â”€ *.csv
â”œâ”€â”€ ticker_analysis/
â”‚   â””â”€â”€ *.csv
â”œâ”€â”€ positive_news/
â”‚   â””â”€â”€ *.csv
â””â”€â”€ negative_news/
    â””â”€â”€ *.csv
```

### Copier depuis le conteneur (si nÃ©cessaire) :

```powershell
docker cp spark-master:/data/spark-output C:\Users\benal\spark-tp\
```

---

## ğŸ“Š Visualisation des RÃ©sultats

### Avec Pandas (Python)

```python
import pandas as pd

# Charger un rÃ©sultat CSV
df = pd.read_csv(r"C:\Users\benal\spark-tp\spark-output\sentiment_by_source_csv\part-00000-*.csv")
print(df.head())
```

### Avec Excel

Ouvrir directement les fichiers `.csv` depuis `C:\Users\benal\spark-tp\spark-output\`.

---

## ğŸ› DÃ©pannage

### âŒ Erreur "No module named 'pyspark'"

PySpark manquant â†’ Installer :
```powershell
pip install pyspark==3.5.0
```

### âŒ "Connection refused" Ã  Elasticsearch

VÃ©rifier que les conteneurs tournent :
```powershell
docker compose ps
```

VÃ©rifier que Elasticsearch est accessible :
```powershell
curl http://localhost:9201/_cluster/health?pretty
```

### âŒ "Index not found" ou "0 documents"

Le producer n'a pas encore envoyÃ© de donnÃ©es. Attendre quelques minutes ou vÃ©rifier :
```powershell
curl "http://localhost:9201/market-news-*/_count?pretty"
```

### âŒ Spark Master inaccessible

VÃ©rifier le nom du master dans `job.py` ligne 30 :
```python
SPARK_MASTER = "spark://spark-master:7077"  # Nom selon docker-compose
```

---

## ğŸ“¸ Captures d'Ã©cran pour le Rapport

Pour le livrable Partie 5 (20 pts), capturer :

1. âœ… Commande d'exÃ©cution du job Spark
2. âœ… Logs de sortie (5 analyses)
3. âœ… Fichiers CSV/JSON gÃ©nÃ©rÃ©s
4. âœ… Contenu d'au moins 2 fichiers (ex: sentiment_by_source, ticker_analysis)
5. âœ… Graphiques depuis Excel ou Pandas (optionnel mais valorisÃ©)

---

## ğŸ¯ IntÃ©gration avec Kibana (Partie 4)

Les rÃ©sultats Spark peuvent Ãªtre **rÃ©indexÃ©s dans Elasticsearch** pour visualisation dans Kibana :

```python
# Dans job.py, ajouter Ã  la fin :
sentiment_by_source.write \
    .format("org.elasticsearch.spark.sql") \
    .option("es.nodes", ES_HOST) \
    .option("es.port", ES_PORT) \
    .option("es.resource", "spark-analysis/_doc") \
    .mode("overwrite") \
    .save()
```

Puis crÃ©er un nouveau Data View dans Kibana : `spark-analysis`.

---

## ğŸ“ Justification Technique (pour le rapport)

### Pourquoi Spark ?

1. **Traitement distribuÃ©** : mÃªme si les donnÃ©es sont faibles, Spark dÃ©montre la capacitÃ© Ã  scaler
2. **Connecteur natif** : `elasticsearch-spark` permet lecture/Ã©criture directe
3. **Transformations puissantes** : `explode`, `groupBy`, `agg` pour analyses complexes
4. **Export multi-format** : JSON (rÃ©ingestion), CSV (Excel, Pandas)

### Alternatives considÃ©rÃ©es

- **Hadoop MapReduce** : plus verbeux, moins adaptÃ© aux analyses interactives
- **Pandas** : limitÃ© au single-node, pas de distribution
- **Spark** âœ… : compromis idÃ©al pour le TP

---

## âœ… Checklist Partie 5

- [ ] Job Spark exÃ©cutÃ© sans erreur
- [ ] 5 analyses complÃ©tÃ©es
- [ ] RÃ©sultats exportÃ©s en CSV + JSON
- [ ] Captures d'Ã©cran prises
- [ ] Documentation technique rÃ©digÃ©e
- [ ] Justification des choix techniques (pourquoi Spark ?)

---

## ğŸ“ Support

En cas de problÃ¨me, vÃ©rifier :
1. Logs Docker : `docker compose logs -f`
2. Logs Spark : dans la sortie de `spark-submit`
3. Ã‰tat Elasticsearch : `curl http://localhost:9201/_cat/indices?v`

**Environnement validÃ©** : Windows 10/11 + Docker Desktop + Git Bash
