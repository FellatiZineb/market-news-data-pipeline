# ğŸ¯ PLAN D'ACTION - Parties 4 et 5

## ğŸ“Œ Contexte

Ta binÃ´me a terminÃ© les **Parties 1, 2 et 3** (Collecte API, Kafka, Logstash, Elasticsearch).

**Tes responsabilitÃ©s** :
- âœ… **Partie 4** : RequÃªtes Elasticsearch + Visualisations Kibana (40 pts)
- âœ… **Partie 5** : Traitement distribuÃ© Spark (20 pts)

---

## â±ï¸ Timeline RecommandÃ©e (8-10h de travail)

### Jour 1 (3h) : Configuration + Collecte de DonnÃ©es
- [x] DÃ©marrer Docker Compose âœ…
- [x] Lancer le producer Python âœ…
- [ ] **Laisser tourner 2-3 heures** pour collecter assez de donnÃ©es (~100-200 articles)
- [ ] VÃ©rifier l'indexation Elasticsearch

### Jour 2 (3h) : Partie 4 - Elasticsearch + Kibana
- [ ] Tester les 5 requÃªtes Elasticsearch
- [ ] CrÃ©er le Data View dans Kibana
- [ ] CrÃ©er les 6 visualisations
- [ ] Assembler le dashboard
- [ ] Prendre les captures d'Ã©cran

### Jour 3 (2h) : Partie 5 - Spark
- [ ] ExÃ©cuter le job Spark
- [ ] VÃ©rifier les exports CSV/JSON
- [ ] Prendre les captures d'Ã©cran
- [ ] RÃ©diger la justification technique

### Jour 4 (2h) : Rapport Final
- [ ] Compiler toutes les captures
- [ ] RÃ©diger les explications
- [ ] Relire et structurer le document
- [ ] Soumettre avant le 27 fÃ©vrier

---

## ğŸš€ Ã‰TAPES CONCRÃˆTES

### MAINTENANT : Collecte de DonnÃ©es (30 min - 3h)

Le producer Python tourne et collecte des donnÃ©es. **Il faut attendre 2-3 heures** pour avoir assez de donnÃ©es (100-200 articles minimum).

#### Action immÃ©diate :

1. **VÃ©rifier que le producer tourne** (fenÃªtre PowerShell ouverte avec `producer.py`)
   - Si pas lancÃ© : `cd producer` â†’ `python producer.py`
   - Ne PAS fermer la fenÃªtre

2. **Pendant que Ã§a tourne, lire les guides :**
   - [elasticsearch-queries/QUERIES.md](elasticsearch-queries/QUERIES.md)
   - [kibana/VISUALIZATIONS.md](kibana/VISUALIZATIONS.md)
   - [spark/README.md](spark/README.md)

3. **VÃ©rifier toutes les 30 min** que des donnÃ©es arrivent :
   ```powershell
   curl "http://localhost:9201/market-news-*/_count?pretty"
   ```
   
   **Objectif** : avoir au moins **50-100 documents** avant de commencer les visualisations.

---

### Ã‰TAPE 1 : Partie 4.1 - RequÃªtes Elasticsearch (1h)

**PrÃ©requis** : Au moins 50 documents indexÃ©s

#### 1.1 Ouvrir Kibana Dev Tools

- URL : http://localhost:5601
- Menu â†’ **Dev Tools**

#### 1.2 Copier-coller les 5 requÃªtes

Depuis [elasticsearch-queries/QUERIES.md](elasticsearch-queries/QUERIES.md) :

1. **RequÃªte textuelle** (Match query)
2. **AgrÃ©gation** (Sentiment moyen par source)
3. **N-gram** (Recherche partielle)
4. **Fuzzy** (TolÃ©rance fautes)
5. **SÃ©rie temporelle** (Date histogram)

#### 1.3 Captures d'Ã©cran

Pour **chaque requÃªte** :
- âœ… Capture de la requÃªte JSON dans Dev Tools
- âœ… Capture du rÃ©sultat JSON (au moins les premiers Ã©lÃ©ments)

**Format** : PNG, haute rÃ©solution

---

### Ã‰TAPE 2 : Partie 4.2 - Visualisations Kibana (2h)

Suivre le guide complet : [kibana/VISUALIZATIONS.md](kibana/VISUALIZATIONS.md)

#### 2.1 CrÃ©er le Data View (5 min)
- Stack Management â†’ Data Views
- Nom : `Market News`
- Index pattern : `market-news-*`
- Timestamp : `@timestamp`

#### 2.2 CrÃ©er 6 visualisations (1h)

1. **Line chart** : Sentiment over time
2. **Horizontal bar** : Avg sentiment by source
3. **Pie chart** : Sentiment distribution
4. **Vertical bar** : Top sources by volume
5. **Metric** : KPIs (total articles, avg sentiment)
6. **Data table** : Latest news

#### 2.3 CrÃ©er le Dashboard (30 min)
- Dashboard â†’ Create new
- Add from library â†’ sÃ©lectionner les 6 visualisations
- Organiser la disposition
- Ajouter des filtres (Time range, Tickers, etc.)

#### 2.4 Captures d'Ã©cran (15 min)
- âœ… Chaque visualisation individuellement
- âœ… Dashboard complet
- âœ… Dashboard avec diffÃ©rents filtres appliquÃ©s

---

### Ã‰TAPE 3 : Partie 5 - Spark (1h30)

Suivre le guide : [spark/README.md](spark/README.md)

#### 3.1 VÃ©rifier l'environnement Spark

Selon ta fiche technique, tu as dÃ©jÃ  Spark installÃ©. VÃ©rifier :

```powershell
# Si Spark via Docker
docker ps | Select-String spark

# Si Spark local
python -c "import pyspark; print(pyspark.__version__)"
```

#### 3.2 ExÃ©cuter le job Spark (30 min)

```powershell
cd spark
python job.py
```

**Sortie attendue** :
- Connexion Ã  Elasticsearch rÃ©ussie
- 5 analyses complÃ©tÃ©es
- Exports CSV/JSON crÃ©Ã©s

#### 3.3 VÃ©rifier les rÃ©sultats (15 min)

**Emplacement** : Selon ton `OUTPUT_DIR` dans `job.py`

Fichiers attendus :
```
spark-output/
â”œâ”€â”€ global_stats/
â”œâ”€â”€ sentiment_by_source_csv/
â”œâ”€â”€ sentiment_by_source_json/
â”œâ”€â”€ sentiment_distribution/
â”œâ”€â”€ ticker_analysis/
â”œâ”€â”€ positive_news/
â””â”€â”€ negative_news/
```

Ouvrir au moins 2 fichiers CSV avec Excel ou VSCode pour vÃ©rifier le contenu.

#### 3.4 Captures d'Ã©cran (15 min)
- âœ… Commande d'exÃ©cution `python job.py`
- âœ… Logs de sortie (5 analyses)
- âœ… Explorateur de fichiers montrant les CSV/JSON
- âœ… Contenu d'au moins 2 fichiers (ex: `sentiment_by_source.csv`, `ticker_analysis.csv`)

#### 3.5 Justification technique (30 min)

CrÃ©er un document `spark/JUSTIFICATION.md` qui explique :

1. **Pourquoi Spark ?**
   - Calculs distribuÃ©s (mÃªme si donnÃ©es faibles, dÃ©montre la capacitÃ© Ã  scaler)
   - Connecteur natif Elasticsearch
   - Transformations puissantes (`explode`, `groupBy`, `agg`)

2. **Alternatives considÃ©rÃ©es**
   - Hadoop MapReduce : trop verbeux
   - Pandas : pas de distribution
   - **Spark âœ…** : meilleur compromis

3. **Choix techniques**
   - PySpark (Python) plutÃ´t que Scala : plus accessible
   - Lecture directe depuis ES via `elasticsearch-spark`
   - Export multi-format (CSV pour Excel, JSON pour rÃ©ingestion)

---

### Ã‰TAPE 4 : Compilation du Rapport (2h)

#### Structure recommandÃ©e :

```
RAPPORT_PROJET_MARKET_NEWS.pdf

1. Introduction
   - Objectif du projet
   - Architecture globale

2. Partie 1-3 (Travail de ta binÃ´me)
   - Collecte API
   - Kafka (producteur/consommateur)
   - Logstash + Elasticsearch

3. Partie 4 : RequÃªtes Elasticsearch + Kibana
   - 5 requÃªtes JSON + rÃ©sultats
   - 6 visualisations
   - Dashboard complet
   - Captures d'Ã©cran

4. Partie 5 : Traitement Spark
   - Description du job
   - 5 analyses effectuÃ©es
   - RÃ©sultats CSV/JSON
   - Justification technique
   - Captures d'Ã©cran

5. Documentation & Organisation
   - Structure du projet
   - README
   - Commentaires code
   - Choix techniques

6. Conclusion
   - RÃ©sultats obtenus
   - DifficultÃ©s rencontrÃ©es
   - AmÃ©liorations possibles

Annexes :
   - Lien GitHub
   - Fichiers de configuration
   - RÃ©sultats complets (CSV/JSON)
```

---

## ğŸ“‹ Checklist BarÃ¨me (100 pts)

### Partie 1 : Collecte API (10 pts) âœ… BinÃ´me
- [x] Script producer.py
- [x] Exemple de donnÃ©es extraites

### Partie 2 : Kafka (15 pts) âœ… BinÃ´me
- [x] Topic `market_news` crÃ©Ã©
- [x] Producteur configurÃ©
- [x] Consommateur (Logstash) configurÃ©
- [x] Captures d'Ã©cran

### Partie 3 : Logstash & Elasticsearch (25 pts) âœ… BinÃ´me
- [x] Fichier `pipeline.conf`
- [x] Mapping avec analyzers + n-gram
- [x] DonnÃ©es indexÃ©es
- [x] 5 requÃªtes prÃ©parÃ©es

### Partie 4 : Kibana (20 pts) âš ï¸ TOI
- [ ] Data View crÃ©Ã©
- [ ] 6 visualisations crÃ©Ã©es
- [ ] Dashboard complet
- [ ] Captures d'Ã©cran haute qualitÃ©
- [ ] RequÃªtes ES testÃ©es

### Partie 5 : Spark (20 pts) âš ï¸ TOI
- [ ] Job Spark exÃ©cutÃ©
- [ ] 5 analyses complÃ©tÃ©es
- [ ] RÃ©sultats CSV/JSON exportÃ©s
- [ ] Justification technique rÃ©digÃ©e
- [ ] Captures d'Ã©cran

### Documentation (10 pts) âš ï¸ TOI + BinÃ´me
- [ ] README complet
- [ ] Structure claire
- [ ] Commentaires code
- [ ] Rapport PDF final

---

## ğŸ¯ RÃ‰SUMÃ‰ : Ce qui a Ã©tÃ© FAIT pour Toi

âœ… **5 requÃªtes Elasticsearch prÃªtes Ã  l'emploi**
   â†’ Fichier : [elasticsearch-queries/QUERIES.md](elasticsearch-queries/QUERIES.md)

âœ… **Job Spark complet (300+ lignes)**
   â†’ Fichier : [spark/job.py](spark/job.py)

âœ… **Guide visualisations Kibana Ã©tape par Ã©tape**
   â†’ Fichier : [kibana/VISUALIZATIONS.md](kibana/VISUALIZATIONS.md)

âœ… **Documentation Spark complÃ¨te**
   â†’ Fichier : [spark/README.md](spark/README.md)

âœ… **README principal mis Ã  jour**
   â†’ Fichier : [README.md](README.md)

---

## ğŸš¨ Points d'Attention

### âš ï¸ Port Elasticsearch : 9201 (pas 9200)
Dans tous les scripts et commandes curl, utiliser **9201**.

### âš ï¸ Attendre assez de donnÃ©es
Ne pas crÃ©er les visualisations Kibana avec seulement 10 documents. Minimum recommandÃ© : **50-100 documents**.

### âš ï¸ Time Range dans Kibana
Par dÃ©faut, Kibana affiche seulement les 15 derniÃ¨res minutes. Penser Ã  Ã©largir Ã  **Last 30 days** en haut Ã  droite.

### âš ï¸ Spark Master URL
Si erreur de connexion Spark, vÃ©rifier dans `job.py` ligne 30 :
```python
SPARK_MASTER = "spark://spark-master:7077"  # Adapter selon ton env
```

---

## ğŸ“ Si ProblÃ¨me

### Le producer ne dÃ©marre pas
â†’ VÃ©rifier Kafka : `docker compose ps`  
â†’ Nettoyer : `docker compose down -v` puis `docker compose up -d`

### Pas de donnÃ©es dans Elasticsearch
â†’ VÃ©rifier Logstash : `docker compose logs logstash -f`  
â†’ Attendre plus longtemps (le producer poll toutes les heures)

### Kibana affiche "No data"
â†’ VÃ©rifier le Time Range (Last 30 days)  
â†’ Recharger le Data View : Management â†’ Data Views â†’ Refresh

### Spark ne trouve pas Elasticsearch
â†’ VÃ©rifier que le conteneur ES tourne : `docker compose ps`  
â†’ Dans `job.py`, ES_HOST doit Ãªtre `"elasticsearch"` (nom du service Docker)

---

## âœ… Tu es PrÃªt !

Tout est en place. Il ne te reste plus qu'Ã  :

1. **Attendre 2-3h** que le producer collecte des donnÃ©es
2. **Tester les 5 requÃªtes ES** â†’ 30 min
3. **CrÃ©er les visualisations Kibana** â†’ 2h
4. **ExÃ©cuter le job Spark** â†’ 1h
5. **Compiler le rapport** â†’ 2h

**Total estimÃ©** : 5-6h de travail effectif (hors temps de collecte des donnÃ©es)

---

## ğŸ“ Bon Courage !

Si tu suis ce plan Ã©tape par Ã©tape, tu auras **60 points sur 60** pour tes parties 4 et 5.

N'oublie pas de **capturer les Ã©crans** au fur et Ã  mesure, c'est plus facile que de tout refaire Ã  la fin ! ğŸ“¸
