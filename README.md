# ğŸ“Š Market News Data Pipeline

> Pipeline de donnÃ©es temps rÃ©el pour l'analyse de sentiments financiers  
> **M2 DataScale - UniversitÃ© Paris-Saclay**

## ğŸ—ï¸ Architecture

```
Alpha Vantage API â†’ Kafka â†’ Logstash â†’ Elasticsearch â†’ Kibana
                                              â†“
                                           Spark
```

**Technologies** :
- ğŸ“¡ **Producer** : Python + Kafka Producer
- ğŸš€ **Streaming** : Apache Kafka + Zookeeper
- ğŸ”„ **Transformation** : Logstash
- ğŸ” **Indexation** : Elasticsearch (8.12.2)
- ğŸ“Š **Visualisation** : Kibana
- âš¡ **Traitement DistribuÃ©** : Apache Spark

---

## ğŸ“‹ Ã‰tat du Projet

### âœ… Parties ComplÃ¨tes (BinÃ´me)

- **Partie 1** : Collecte API Alpha Vantage âœ…
- **Partie 2** : Transmission Kafka (producteur/consommateur) âœ…
- **Partie 3** : Transformation Logstash + Indexation Elasticsearch âœ…

### ğŸš§ Parties en Cours

- **Partie 4** : RequÃªtes Elasticsearch + Visualisations Kibana (voir [kibana/VISUALIZATIONS.md](kibana/VISUALIZATIONS.md))
- **Partie 5** : Traitement distribuÃ© Spark (voir [spark/README.md](spark/README.md))

---

## ğŸš€ DÃ©marrage Rapide

### 1ï¸âƒ£ PrÃ©requis

- Docker Desktop installÃ© et dÃ©marrÃ©
- Python 3.11+ avec pip
- ClÃ© API Alpha Vantage (gratuite : https://www.alphavantage.co/support/#api-key)

### 2ï¸âƒ£ Configuration

**CrÃ©er le fichier `.env` dans `producer/` :**
```env
ALPHAVANTAGE_API_KEY=VOTRE_CLE_ICI
```

**Installer les dÃ©pendances Python :**
```powershell
cd producer
python -m venv .venv
.\.venv\Scripts\Activate.ps1  # PowerShell
# OU
source .venv/bin/activate      # Git Bash

pip install -r requirements.txt
```

### 3ï¸âƒ£ Lancer la Stack Docker

```powershell
# Depuis la racine du projet
docker compose up -d
```

**VÃ©rifier que tout tourne :**
```powershell
docker compose ps
```

Vous devriez voir 5 conteneurs **Up** :
- `zookeeper`
- `kafka`
- `elasticsearch`
- `kibana`
- `logstash`

**Attendre 30 secondes** que tous les services dÃ©marrent complÃ¨tement.

### 4ï¸âƒ£ Lancer le Producer

```powershell
cd producer
python producer.py
```

**Sortie attendue :**
```
API returned 50 articles
published 15 docs to market_news
```

Le producer tourne en continu (polling toutes les heures). Laisser tourner **au moins 10 minutes** pour accumuler des donnÃ©es.

---

## ğŸ” VÃ©rification des DonnÃ©es

### VÃ©rifier Kafka
```powershell
docker compose exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

Doit afficher : `market_news`

### VÃ©rifier Elasticsearch
```powershell
curl "http://localhost:9201/market-news-*/_count?pretty"
```

Doit retourner un count > 0.

### VÃ©rifier Kibana
Ouvrir : http://localhost:5601

---

## ğŸ“Š Partie 4 : RequÃªtes + Visualisations Kibana

**Guide complet** : [kibana/VISUALIZATIONS.md](kibana/VISUALIZATIONS.md)  
**RequÃªtes Elasticsearch** : [elasticsearch-queries/QUERIES.md](elasticsearch-queries/QUERIES.md)

### RequÃªtes Ã  RÃ©aliser (5 obligatoires)

1. âœ… **RequÃªte textuelle** : Match query sur `title`
2. âœ… **AgrÃ©gation** : Sentiment moyen par source
3. âœ… **N-gram** : Recherche avec `title.ngram`
4. âœ… **Fuzzy** : TolÃ©rance aux fautes de frappe
5. âœ… **SÃ©rie temporelle** : Ã‰volution du sentiment par jour

### Visualisations Ã  CrÃ©er

- Line chart : Ã©volution temporelle
- Bar chart : sentiment par source
- Pie chart : distribution Bullish/Bearish/Neutral
- MÃ©triques : KPIs globaux
- Data table : derniers articles

**Dashboard final** : combiner toutes les visualisations.

---

## âš¡ Partie 5 : Traitement Spark

**Guide complet** : [spark/README.md](spark/README.md)

### Ce que fait le Job Spark

Le script `spark/job.py` effectue **5 analyses distribuÃ©es** :

1. **Statistiques globales** : avg, min, max du sentiment
2. **Sentiment par source** : agrÃ©gation par mÃ©dia
3. **Distribution des labels** : Bullish vs Bearish vs Neutral
4. **Analyse par ticker** : AAPL, MSFT, GOOGL
5. **Top articles** : plus positifs et plus nÃ©gatifs

### ExÃ©cution

```powershell
cd spark
python job.py
```

**RÃ©sultats exportÃ©s** en CSV + JSON dans `.venv/spark-tp/spark-output/` (Ã  adapter selon ton environnement).

---

## ğŸ“‚ Structure du Projet

```
market-news-data-pipeline/
â”œâ”€â”€ docker-compose.yml              # Stack complÃ¨te (Kafka, ES, Kibana, Logstash)
â”œâ”€â”€ README.md                       # Ce fichier
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer.py                 # Collecte API â†’ Kafka
â”‚   â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”‚   â””â”€â”€ .env                        # ClÃ© API (non versionnÃ©)
â”‚
â”œâ”€â”€ elastic/
â”‚   â””â”€â”€ index-template.json         # Mapping ES avec n-gram analyzer
â”‚
â”œâ”€â”€ logstash/
â”‚   â””â”€â”€ pipeline.conf               # Kafka â†’ Elasticsearch
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ job.py                      # Analyses distribuÃ©es
â”‚   â””â”€â”€ README.md                   # Guide d'utilisation Spark
â”‚
â”œâ”€â”€ elasticsearch-queries/
â”‚   â””â”€â”€ QUERIES.md                  # 5 requÃªtes ES obligatoires
â”‚
â””â”€â”€ kibana/
    â””â”€â”€ VISUALIZATIONS.md           # Guide visualisations + dashboard
```

---

## ğŸ”§ Configuration SystÃ¨me

### Ports UtilisÃ©s

| Service         | Port Host | Port Container | AccÃ¨s |
|-----------------|-----------|---|----------|
| Kafka           | 9092      | 9092 | `localhost:9092` |
| Zookeeper       | 2181      | 2181 | `localhost:2181` |
| **Elasticsearch** | **9201** | **9200** | http://localhost:9201 (host) / http://elasticsearch:9200 (Docker) |
| Kibana          | 5601      | 5601 | http://localhost:5601 |
| Logstash        | 9600      | 9600 | `localhost:9600` |

### âš ï¸ Important : Elasticsearch sur 2 ports

- **Depuis HOST (Windows)** : `http://localhost:9201`
  - UtilisÃ© par : curl, Kibana, requÃªtes manuelles
  - Exemple : `curl http://localhost:9201/market-news-*/_count?pretty`

- **Depuis CONTENEURS (Docker network)** : `http://elasticsearch:9200`
  - UtilisÃ© par : Logstash, Spark (dans leurs conteneurs)
  - Exemple dans `job.py` : `ES_HOST = "elasticsearch"`, `ES_PORT = "9200"`

**RÃ©sumÃ©** : Si tu utilises Spark / Logstash EN LOCAL (Windows), dÃ©finis les variables d'environnement :
```powershell
$env:ES_HOST = "localhost"
$env:ES_PORT = "9201"
```

---

## ğŸ› DÃ©pannage

### âŒ Erreur "NoBrokersAvailable"
â†’ Kafka n'est pas dÃ©marrÃ©. Lancer `docker compose up -d` et attendre 30s.

### âŒ "Connection refused" Elasticsearch
â†’ VÃ©rifier que le conteneur tourne : `docker compose ps`  
â†’ Utiliser le bon port : **9201**

### âŒ Pas de donnÃ©es dans Elasticsearch
â†’ Laisser tourner le producer au moins 10 minutes  
â†’ VÃ©rifier les logs Logstash : `docker compose logs logstash -f`

### âŒ Conteneur Kafka crash au dÃ©marrage
â†’ Nettoyer et redÃ©marrer :
```powershell
docker compose down -v
docker compose up -d
```

---

## ğŸ“¸ Livrables pour le Rapport

### Partie 1-3 (DÃ©jÃ  fait par ta binÃ´me)
- Captures des topics Kafka
- Logs du producer
- Mapping Elasticsearch
- DonnÃ©es indexÃ©es

### Partie 4 (Ã€ faire)
- [ ] 5 requÃªtes Elasticsearch exÃ©cutÃ©es + rÃ©sultats JSON
- [ ] Captures des visualisations Kibana
- [ ] Dashboard complet

### Partie 5 (Ã€ faire)
- [ ] Logs d'exÃ©cution du job Spark
- [ ] Fichiers CSV/JSON exportÃ©s
- [ ] Justification technique (pourquoi Spark ?)

### Documentation
- [ ] README complet (ce fichier)
- [ ] Commentaires dans le code
- [ ] Explications des choix techniques

---

## ğŸ“š Ressources

- [Alpha Vantage API Docs](https://www.alphavantage.co/documentation/)
- [Kafka Python Client](https://kafka-python.readthedocs.io/)
- [Elasticsearch Python Client](https://elasticsearch-py.readthedocs.io/)
- [Kibana Lens](https://www.elastic.co/guide/en/kibana/current/lens.html)
- [PySpark SQL](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)

---

## ğŸ‘¥ Contributeurs

- **BinÃ´me** : Parties 1, 2, 3 (Collecte, Kafka, Logstash, Elasticsearch)
- **Toi** : Parties 4, 5 (RequÃªtes, Kibana, Spark)

**Projet** : M2 DataScale - UniversitÃ© Paris-Saclay  
**Date limite** : 27 fÃ©vrier 2026

---

## âœ… Checklist Finale

- [x] Partie 1 : API + Producer Kafka
- [x] Partie 2 : Topics Kafka + Consommation
- [x] Partie 3 : Logstash + Mapping ES
- [ ] Partie 4 : 5 requÃªtes ES + Visualisations Kibana
- [ ] Partie 5 : Job Spark + Export rÃ©sultats
- [ ] Documentation complÃ¨te + Captures

---

## ğŸ¯ Prochaines Ã‰tapes IMMÃ‰DIATES

1. **Laisser tourner le producer** 10-15 minutes pour collecter des donnÃ©es
2. **VÃ©rifier l'indexation** : `curl "http://localhost:9201/market-news-*/_count?pretty"`
3. **Tester les 5 requÃªtes ES** : voir [elasticsearch-queries/QUERIES.md](elasticsearch-queries/QUERIES.md)
4. **CrÃ©er les visualisations Kibana** : voir [kibana/VISUALIZATIONS.md](kibana/VISUALIZATIONS.md)
5. **ExÃ©cuter le job Spark** : voir [spark/README.md](spark/README.md)
6. **Capturer les rÃ©sultats** pour le rapport final
